{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ** Описание **"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Построить графики распределения в спам и не спам множествах следующих признаков:\n",
    "\n",
    "1\tКоличество слов на странице\n",
    "2\tСредняя длинна слова\n",
    "3\tКоличество слов в заголовке страниц (слова в теге <html><head><title> Some text </title>)\n",
    "4\tКоличество слов в анкорах ссылок (<html><body><a> Some text </a>)\n",
    "5\tКоэффициент сжатия\n",
    "\n",
    "Нужно посчитать статистику минимум по трем признакам и обязательно сделать для 1-го и 2-го признаков\n",
    "\n",
    "И отправить первое решение в соревнование https://kaggle.com/join/antispam_infopoisk\n",
    "На основании одного из указанных выше признаков попытаться разделить мн-во, так чтобы score в соревновании был больше 0.55\n",
    "\n",
    "При выполнении всех этих условия в течении семинара +1 балл к ДЗ\n",
    "\n",
    "Описание ДЗ и правил выставления за него баллов в https://inclass.kaggle.com/c/antispam-infopoisk  \n",
    "Сроки ДЗ уточнить у преподователя - обычно 2 недели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import base64\n",
    "import csv\n",
    "import gzip\n",
    "import zlib\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACE_NUM = 1000\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%H:%M:%S')\n",
    "\n",
    "def trace(items_num, trace_num=TRACE_NUM):\n",
    "    if items_num % trace_num == 0: logging.info(\"Complete items %05d\" % items_num)\n",
    "        \n",
    "def trace_worker(items_num, worker_id, trace_num=TRACE_NUM):\n",
    "    if items_num % trace_num == 0: logging.info(\"Complete items %05d in worker_id %d\" % (items_num, worker_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Утилиты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Декораторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_utf8(text):\n",
    "    if isinstance(text, unicode): text = text.encode('utf8')\n",
    "    return text\n",
    "\n",
    "def convert2unicode(f):\n",
    "    def tmp(text):\n",
    "        if not isinstance(text, unicode): text = text.decode('utf8')\n",
    "        return f(text)\n",
    "    return tmp\n",
    "\n",
    "def convert2lower(f):\n",
    "    def tmp(text):        \n",
    "        return f(text.lower())\n",
    "    return tmp\n",
    "\n",
    "#P.S. Декораторы могут усложнять отладку, так что от них вполне можно отказаться и воспользоваться copy-paste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Извлечение текста из html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Извлечение текста при помощи встроенных модулей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "import re\n",
    "\n",
    "###Извлечение текста из title можно вписать сюда\n",
    "\n",
    "class TextHTMLParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        HTMLParser.__init__(self)\n",
    "        self._text = []\n",
    "        self._title = []\n",
    "        self._link = []\n",
    "        self._in_title = False\n",
    "        self._in_link = False\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        text = data.strip()\n",
    "        if len(text) > 0:\n",
    "            text = re.sub('[ \\t\\r\\n]+', ' ', text)\n",
    "            if self._in_title:\n",
    "                self._title.append(text + ' ')\n",
    "            if self._in_link:\n",
    "                self._link.append(text + ' ')\n",
    "            self._text.append(text + ' ')\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'p':\n",
    "            self._text.append('\\n\\n')\n",
    "        elif tag == 'br':\n",
    "            self._text.append('\\n')\n",
    "        elif tag == 'title':\n",
    "            self._in_title = True\n",
    "        elif tag == 'a':\n",
    "            self._in_link = True\n",
    "            \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == 'title':\n",
    "            self._in_title = False\n",
    "        elif tag == 'a':\n",
    "            self._in_link = False\n",
    "\n",
    "    def handle_startendtag(self, tag, attrs):\n",
    "        if tag == 'br':\n",
    "            self._text.append('\\n\\n')\n",
    "\n",
    "    def text(self):\n",
    "        return ''.join(self._text).strip()\n",
    "    \n",
    "    def title(self):\n",
    "        return ''.join(self._title).strip()\n",
    "    \n",
    "    def link(self):\n",
    "        return ''.join(self._link).strip()\n",
    "\n",
    "@convert2unicode\n",
    "def html2text_parser(text):\n",
    "    parser = TextHTMLParser()\n",
    "    parser.feed(text)\n",
    "    return parser.text(), parser.title(), parser.link()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Извлечение текста при помощи дополнительных библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html2text_bs(raw_html):\n",
    "    from bs4 import BeautifulSoup\n",
    "    \"\"\"\n",
    "    Тут производится извлечения из html текста\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    [s.extract() for s in soup(['script', 'style'])]\n",
    "    return soup.get_text()\n",
    "\n",
    "def html2text_bs_visible(raw_html):\n",
    "    from bs4 import BeautifulSoup\n",
    "    \"\"\"\n",
    "    Тут производится извлечения из html текста, который видим пользователю\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")    \n",
    "    [s.extract() for s in soup(['style', 'script', '[document]', 'head', 'title'])]\n",
    "    return soup.get_text()\n",
    "\n",
    "def html2text_boilerpipe(raw_html):\n",
    "    import boilerpipe\n",
    "    \"\"\"\n",
    "    еще одна библиотека очень хорошо извлекающая именно видимый пользователю текст,\n",
    "    но она завязана на java\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выбираем какой метод для конвертации html в текст будет основным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#html2text = html2text_bs\n",
    "html2text = html2text_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Методы для токенизации текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@convert2lower\n",
    "@convert2unicode\n",
    "def easy_tokenizer(text):\n",
    "    word = unicode()\n",
    "    for symbol in text:\n",
    "        if symbol.isalnum(): word += symbol\n",
    "        elif word:\n",
    "            yield word\n",
    "            word = unicode()\n",
    "    if word: yield word\n",
    "\n",
    "PYMORPHY_CACHE = {}\n",
    "MORPH = None\n",
    "#hint, чтобы установка pymorphy2 не была бы обязательной\n",
    "def get_lemmatizer():\n",
    "    import pymorphy2\n",
    "    global MORPH\n",
    "    if MORPH is None: MORPH = pymorphy2.MorphAnalyzer()\n",
    "    return MORPH\n",
    "\n",
    "@convert2lower\n",
    "@convert2unicode\n",
    "def pymorphy_tokenizer(text):\n",
    "    global PYMORPHY_CACHE\n",
    "    for word in easy_tokenizer(text):\n",
    "        word_hash = hash(word)\n",
    "        if word_hash not in PYMORPHY_CACHE:\n",
    "            PYMORPHY_CACHE[word_hash] = get_lemmatizer().parse(word)[0].normal_form            \n",
    "        yield PYMORPHY_CACHE[word_hash]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Основная функция, которая вызывается для преобразования html в список слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html2word(raw_html, to_text=html2text, tokenizer=easy_tokenizer):\n",
    "    text, title, link = to_text(raw_html)\n",
    "    return tokenizer(text.lower()), tokenizer(title.lower()), tokenizer(link.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Рассчет финальных метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_divide(a, b):\n",
    "    if a == 0: return 0.0\n",
    "    elif b == 0: return 0.0\n",
    "    else: return a/b\n",
    "\n",
    "def calculate_metrics(predictions, threshold):    \n",
    "    \"\"\"\n",
    "    Функция подсчета метрик\n",
    "    Параметры\n",
    "    predictions - ранки по документам\n",
    "    threshold  - порог для метрик\n",
    "    \"\"\"\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    true_negative = 0\n",
    "    false_negative = 0\n",
    "    for (url_id, mark, url, prediction) in predictions:        \n",
    "        mark_predict = prediction > threshold\n",
    "\n",
    "        if mark_predict:                     \n",
    "            if mark_predict == mark: true_positive += 1\n",
    "            else: false_positive += 1                    \n",
    "        else:                     \n",
    "            if  mark_predict == mark: true_negative += 1\n",
    "            else: false_negative += 1\n",
    "\n",
    "    class_prec  = safe_divide(true_positive, true_positive + false_positive)\n",
    "    class_recall = safe_divide(true_positive, true_positive + false_negative)\n",
    "        \n",
    "    class_F1 = safe_divide(2 * class_prec * class_recall, class_prec + class_recall)\n",
    "    \n",
    "    \n",
    "    not_class_prec = safe_divide(true_negative, true_negative + false_negative)\n",
    "    not_class_recall = safe_divide(true_negative, true_negative + false_positive)\n",
    "    \n",
    "    not_class_F1 = safe_divide(2 * not_class_prec * not_class_recall, not_class_prec + not_class_recall)\n",
    "    \n",
    "    return ( (class_prec, class_recall, class_F1), (not_class_prec, not_class_recall, not_class_F1) )\n",
    "\n",
    "def arange(start, stop, step):\n",
    "    cur_value = start\n",
    "    while True:\n",
    "        if cur_value > stop: break\n",
    "        yield cur_value\n",
    "        cur_value += step\n",
    "\n",
    "def plot_results(docs, min_threshold=-1, max_threshold=1, step=0.1, trace=False):\n",
    "    x = []\n",
    "    y_p = []\n",
    "    y_n = []\n",
    "    docs_predictions = classifier.predict_all(docs)\n",
    "    for threshold in arange(min_threshold, max_threshold, step):\n",
    "        r = calculate_metrics(docs_predictions, threshold)\n",
    "        x.append(threshold)\n",
    "        y_p.append(r[0])\n",
    "        y_n.append(r[1])        \n",
    "        if trace: \n",
    "            print 'threshold %s' % threshold\n",
    "            print '\\tclass_prec %s, class_recall %s, class_F1 %s' % r[0]\n",
    "            print '\\tnot_class_prec %s, not_class_recall %s, not_class_F1 %s' % r[1]\n",
    "            print '\\t\\tMacroF1Mesure %s' % ((r[0][2] + r[1][2])/2)\n",
    "    plot_stats(x, y_p, \"Class Result\")\n",
    "    plot_stats(x, y_n, \"Not class Result\")    \n",
    "\n",
    "\n",
    "def plot_stats(x, y, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    prec, = plt.plot( x, \n",
    "                     [k[0] for k in y], \"r\", label='Precision', \n",
    "                     linewidth=1)\n",
    "    accur, = plt.plot( x, \n",
    "                      [k[1] for k in y], \"b\", label='Recall',\n",
    "                      linewidth=1)\n",
    "    f1, =    plt.plot( x, \n",
    "                      [k[2] for k in y], \"g\", label='F1',\n",
    "                      linewidth=1)\n",
    "    plt.grid(True)\n",
    "    plt.legend(handles=[prec, accur, f1])\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_features(url, html_data):\n",
    "    words, title, link = map(list, html2word(html_data))\n",
    "    words_num = len(words)\n",
    "    avg_word_len = sum([len(word) for word in words])/len(words)\n",
    "    title_words_num = len(title)\n",
    "    anchor_words_num = len(link)\n",
    "    compression_level = len(zlib.compress(to_utf8(html_data)))/len(html_data)\n",
    "    \n",
    "    return [len(words), avg_word_len, title_words_num, anchor_words_num, compression_level, html2text(html_data)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Заголовок Ololo спам 1 2 3 Текст ссылки Текст ссылки 1\n"
     ]
    }
   ],
   "source": [
    "test_html_data = u'''\n",
    "<html>\n",
    "<title> Заголовок Ololo </title>\n",
    "спам 1 2 3\n",
    "<body> <a>Текст ссылки </a> \n",
    "<a>Текст ссылки 1 </a> \n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "test_url = 'http://ololo'\n",
    "test_features = calc_features(test_url, test_html_data)\n",
    "print test_features[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "\n",
    "DocItem = namedtuple('DocItem', ['doc_id', 'is_spam', 'url', 'features'])\n",
    "\n",
    "WORKER_NUM = 4\n",
    "\n",
    "def load_csv(input_file_name, calc_features_f):    \n",
    "    \"\"\"\n",
    "    Загружаем данные и извлекаем на лету признаки\n",
    "    Сам контент не сохраняется, чтобы уменьшить потребление памяти - чтобы\n",
    "    можно было запускать даже на ноутбуках в классе\n",
    "    \"\"\"\n",
    "    \n",
    "    with gzip.open(input_file_name) if input_file_name.endswith('gz') else open(input_file_name)  as input_file:            \n",
    "        headers = input_file.readline()\n",
    "        \n",
    "        for i, line in enumerate(input_file):\n",
    "            trace(i)\n",
    "            parts = line.strip().split('\\t')\n",
    "            url_id = int(parts[0])                                        \n",
    "            mark = bool(int(parts[1]))                    \n",
    "            url = parts[2]\n",
    "            pageInb64 = parts[3]\n",
    "            html_data = base64.b64decode(pageInb64)\n",
    "            features = calc_features_f(url, html_data)            \n",
    "            yield DocItem(url_id, mark, url, features)            \n",
    "                \n",
    "        trace(i, 1)  \n",
    "        \n",
    "def load_csv_worker(input_file_name, calc_features_f, worker_id, res_queue):    \n",
    "    with gzip.open(input_file_name) if input_file_name.endswith('gz') else open(input_file_name)  as input_file:            \n",
    "        headers = input_file.readline()\n",
    "        \n",
    "        for i, line in enumerate(input_file):\n",
    "            trace_worker(i, worker_id)\n",
    "            if i % WORKER_NUM != worker_id: continue\n",
    "            parts = line.strip().split('\\t')\n",
    "            url_id = int(parts[0])                                        \n",
    "            mark = bool(int(parts[1]))                    \n",
    "            url = parts[2]\n",
    "            pageInb64 = parts[3]\n",
    "            html_data = base64.b64decode(pageInb64)\n",
    "            features = calc_features_f(url, html_data)            \n",
    "            res_queue.put(DocItem(url_id, mark, url, features))\n",
    "                \n",
    "        trace_worker(i, worker_id, 1)  \n",
    "    res_queue.put(None)\n",
    "        \n",
    "def load_csv_multiprocess(input_file_name, calc_features_f):\n",
    "    processes = []\n",
    "    res_queue = Queue()    \n",
    "    for i in xrange(WORKER_NUM):\n",
    "        process = Process(target=load_csv_worker, args=(input_file_name, calc_features_f, i, res_queue))\n",
    "        processes.append(process)\n",
    "        process.start()\n",
    "    \n",
    "    complete_workers = 0\n",
    "    while complete_workers != WORKER_NUM:\n",
    "        item = res_queue.get()\n",
    "        if item is None:\n",
    "            complete_workers += 1\n",
    "        else:\n",
    "            yield item\n",
    "        \n",
    "    for process in processes: process.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Обрабатываем входной файл **\n",
    "<br>\n",
    "Формат - поля разделенные табуляциями\n",
    "<br>\n",
    "0 - идентификатор документа\n",
    "<br>\n",
    "1 - метка класса 0 - не спам, 1 - спам\n",
    "<br>\n",
    "2 - урл документа\n",
    "<br>\n",
    "3 - документ в кодировке base64\n",
    "\n",
    "Выходной формат - массив кортежей вида\n",
    "(doc_id, is_spam, url, html_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:33:14 INFO:Complete items 00000 in worker_id 2\n",
      "23:33:14 INFO:Complete items 00000 in worker_id 0\n",
      "23:33:14 INFO:Complete items 00000 in worker_id 3\n",
      "23:33:14 INFO:Complete items 00000 in worker_id 1\n",
      "23:33:56 INFO:Complete items 01000 in worker_id 0\n",
      "23:33:57 INFO:Complete items 01000 in worker_id 2\n",
      "23:33:58 INFO:Complete items 01000 in worker_id 3\n",
      "23:34:00 INFO:Complete items 01000 in worker_id 1\n",
      "23:34:09 INFO:Complete items 02000 in worker_id 0\n",
      "23:34:09 INFO:Complete items 02000 in worker_id 1\n",
      "23:34:10 INFO:Complete items 02000 in worker_id 3\n",
      "23:34:10 INFO:Complete items 02000 in worker_id 2\n",
      "23:34:19 INFO:Complete items 03000 in worker_id 0\n",
      "23:34:19 INFO:Complete items 03000 in worker_id 1\n",
      "23:34:20 INFO:Complete items 03000 in worker_id 3\n",
      "23:34:22 INFO:Complete items 03000 in worker_id 2\n",
      "23:34:32 INFO:Complete items 04000 in worker_id 0\n",
      "23:34:33 INFO:Complete items 04000 in worker_id 1\n",
      "23:34:34 INFO:Complete items 04000 in worker_id 2\n",
      "23:34:35 INFO:Complete items 04000 in worker_id 3\n",
      "23:34:41 INFO:Complete items 05000 in worker_id 0\n",
      "23:34:42 INFO:Complete items 05000 in worker_id 2\n",
      "23:34:42 INFO:Complete items 05000 in worker_id 1\n",
      "23:34:46 INFO:Complete items 05000 in worker_id 3\n",
      "23:34:50 INFO:Complete items 06000 in worker_id 2\n",
      "23:34:50 INFO:Complete items 06000 in worker_id 0\n",
      "23:34:52 INFO:Complete items 06000 in worker_id 1\n",
      "23:34:55 INFO:Complete items 06000 in worker_id 3\n",
      "23:34:59 INFO:Complete items 07000 in worker_id 2\n",
      "23:34:59 INFO:Complete items 07000 in worker_id 0\n",
      "23:34:59 INFO:Complete items 07043 in worker_id 2\n",
      "23:34:59 INFO:Complete items 07043 in worker_id 0\n",
      "23:35:01 INFO:Complete items 07000 in worker_id 1\n",
      "23:35:01 INFO:Complete items 07043 in worker_id 1\n",
      "23:35:05 INFO:Complete items 07000 in worker_id 3\n",
      "23:35:05 INFO:Complete items 07043 in worker_id 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.2 s, sys: 477 ms, total: 1.68 s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TRAIN_DATA_FILE  = 'kaggle_train_data_tab.csv.gz'\n",
    "# TRAIN_DATA_FILE  = 'kaggle/kaggle_train_data_tab_300.csv.gz'\n",
    "\n",
    "train_docs = list(load_csv_multiprocess(TRAIN_DATA_FILE, calc_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_target = [int(doc.is_spam) for doc in train_docs if doc.features != None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('russian')\n",
    "stop_words.extend([u'что', u'это', u'так', u'вот', u'быть', u'как', u'в', u'—', u'к', u'на'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(text):\n",
    "    regex = re.compile(u'[^a-zA-Zа-яА-Я]')\n",
    "    return regex.sub(u' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prep_text = [prep(doc.features[5].lower()) for doc in train_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_prep = TfidfVectorizer(ngram_range=(1,3), min_df=5, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 23s, sys: 7.81 s, total: 2min 31s\n",
      "Wall time: 2min 30s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 3), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=[u'\\u0438', u'\\u0432', u'\\u0432\\u043e', u'\\u043d\\u0435', u'\\u0447\\u0442\\u043e', u'\\u043e\\u043d', u'\\u043d\\u0430', u'\\u044f', u'\\u0441', u'\\u0441\\u043e', u'\\u043a\\u0430\\u043a', u'\\u0430', u'\\u0442\\u043e', u'\\u0432\\u0441\\u0435', u'\\u043e\\u043d\\u0430', u'\\u0442\\u0430\\u043a', u'\\u0435\\u0433\\u...'\\u0431\\u044b\\u0442\\u044c', u'\\u043a\\u0430\\u043a', u'\\u0432', u'\\u2014', u'\\u043a', u'\\u043d\\u0430'],\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "vectorizer_prep.fit(prep_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.3 s, sys: 677 ms, total: 40 s\n",
      "Wall time: 39.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "X_vec_prep = vectorizer_prep.transform(prep_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_prep = xgb.XGBClassifier(n_jobs=-1, max_depth=5, n_estimators=400)\n",
    "\n",
    "lr_prep = LogisticRegression(C=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48min 54s, sys: 3min 7s, total: 52min 2s\n",
      "Wall time: 4min 8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=5, min_child_weight=1, missing=None, n_estimators=400,\n",
       "       n_jobs=-1, nthread=None, objective='binary:logistic',\n",
       "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=True, subsample=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time gb_prep.fit(X_vec_prep, texts_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 26s, sys: 6.28 s, total: 1min 32s\n",
      "Wall time: 8.99 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time lr_prep.fit(X_vec_prep, texts_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:42:46 INFO:Complete items 00000 in worker_id 0\n",
      "23:42:46 INFO:Complete items 00000 in worker_id 1\n",
      "23:42:46 INFO:Complete items 00000 in worker_id 2\n",
      "23:42:46 INFO:Complete items 00000 in worker_id 3\n",
      "23:42:58 INFO:Complete items 01000 in worker_id 2\n",
      "23:43:01 INFO:Complete items 01000 in worker_id 1\n",
      "23:43:06 INFO:Complete items 01000 in worker_id 3\n",
      "23:43:09 INFO:Complete items 01000 in worker_id 0\n",
      "23:43:14 INFO:Complete items 02000 in worker_id 2\n",
      "23:43:17 INFO:Complete items 02000 in worker_id 1\n",
      "23:43:27 INFO:Complete items 02000 in worker_id 0\n",
      "23:43:31 INFO:Complete items 02000 in worker_id 3\n",
      "23:43:31 INFO:Complete items 03000 in worker_id 2\n",
      "23:43:37 INFO:Complete items 03000 in worker_id 1\n",
      "23:43:44 INFO:Complete items 03000 in worker_id 3\n",
      "23:43:45 INFO:Complete items 04000 in worker_id 2\n",
      "23:43:45 INFO:Complete items 03000 in worker_id 0\n",
      "23:43:52 INFO:Complete items 04000 in worker_id 1\n",
      "23:43:58 INFO:Complete items 05000 in worker_id 2\n",
      "23:44:00 INFO:Complete items 04000 in worker_id 0\n",
      "23:44:02 INFO:Complete items 04000 in worker_id 3\n",
      "23:44:08 INFO:Complete items 05000 in worker_id 1\n",
      "23:44:09 INFO:Complete items 06000 in worker_id 2\n",
      "23:44:13 INFO:Complete items 05000 in worker_id 0\n",
      "23:44:14 INFO:Complete items 05000 in worker_id 3\n",
      "23:44:21 INFO:Complete items 07000 in worker_id 2\n",
      "23:44:26 INFO:Complete items 06000 in worker_id 0\n",
      "23:44:27 INFO:Complete items 06000 in worker_id 1\n",
      "23:44:27 INFO:Complete items 06000 in worker_id 3\n",
      "23:44:32 INFO:Complete items 08000 in worker_id 2\n",
      "23:44:36 INFO:Complete items 07000 in worker_id 3\n",
      "23:44:38 INFO:Complete items 07000 in worker_id 1\n",
      "23:44:39 INFO:Complete items 07000 in worker_id 0\n",
      "23:44:45 INFO:Complete items 09000 in worker_id 2\n",
      "23:44:53 INFO:Complete items 08000 in worker_id 3\n",
      "23:44:55 INFO:Complete items 08000 in worker_id 1\n",
      "23:44:57 INFO:Complete items 08000 in worker_id 0\n",
      "23:45:01 INFO:Complete items 10000 in worker_id 2\n",
      "23:45:04 INFO:Complete items 09000 in worker_id 3\n",
      "23:45:09 INFO:Complete items 09000 in worker_id 1\n",
      "23:45:14 INFO:Complete items 09000 in worker_id 0\n",
      "23:45:16 INFO:Complete items 11000 in worker_id 2\n",
      "23:45:25 INFO:Complete items 10000 in worker_id 1\n",
      "23:45:29 INFO:Complete items 10000 in worker_id 0\n",
      "23:45:31 INFO:Complete items 12000 in worker_id 2\n",
      "23:45:33 INFO:Complete items 10000 in worker_id 3\n",
      "23:45:35 INFO:Complete items 11000 in worker_id 1\n",
      "23:45:45 INFO:Complete items 13000 in worker_id 2\n",
      "23:45:45 INFO:Complete items 11000 in worker_id 3\n",
      "23:45:45 INFO:Complete items 11000 in worker_id 0\n",
      "23:45:53 INFO:Complete items 12000 in worker_id 1\n",
      "23:45:57 INFO:Complete items 12000 in worker_id 3\n",
      "23:46:00 INFO:Complete items 12000 in worker_id 0\n",
      "23:46:04 INFO:Complete items 14000 in worker_id 2\n",
      "23:46:06 INFO:Complete items 13000 in worker_id 1\n",
      "23:46:11 INFO:Complete items 13000 in worker_id 3\n",
      "23:46:13 INFO:Complete items 13000 in worker_id 0\n",
      "23:46:21 INFO:Complete items 15000 in worker_id 2\n",
      "23:46:23 INFO:Complete items 14000 in worker_id 1\n",
      "23:46:27 INFO:Complete items 14000 in worker_id 3\n",
      "23:46:28 INFO:Complete items 14000 in worker_id 0\n",
      "23:46:39 INFO:Complete items 15000 in worker_id 1\n",
      "23:46:44 INFO:Complete items 16000 in worker_id 2\n",
      "23:46:45 INFO:Complete items 16038 in worker_id 2\n",
      "23:46:46 INFO:Complete items 15000 in worker_id 0\n",
      "23:46:46 INFO:Complete items 15000 in worker_id 3\n",
      "23:46:58 INFO:Complete items 16000 in worker_id 0\n",
      "23:46:58 INFO:Complete items 16000 in worker_id 1\n",
      "23:46:58 INFO:Complete items 16038 in worker_id 1\n",
      "23:46:58 INFO:Complete items 16038 in worker_id 0\n",
      "23:47:01 INFO:Complete items 16000 in worker_id 3\n",
      "23:47:03 INFO:Complete items 16038 in worker_id 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.71 s, sys: 2.41 s, total: 9.12 s\n",
      "Wall time: 4min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TEST_DATA_FILE  = 'kaggle_test_data_tab.csv.gz'\n",
    "# TEST_DATA_FILE  = 'kaggle/kaggle_train_data_tab_300.csv.gz'\n",
    "\n",
    "test_docs = list(load_csv_multiprocess(TEST_DATA_FILE, calc_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prep = [prep(doc.features[5].lower()) for doc in test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_prep = vectorizer_prep.transform(test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_prep_proba = gb_prep.predict_proba(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_prep_proba = lr_prep.predict_proba(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('my_submission_prep.csv', 'wb') as fout:\n",
    "    writer = csv.writer(fout)\n",
    "    writer.writerow(['Id','Prediction'])\n",
    "    idx = 0\n",
    "    for doc in test_docs:\n",
    "        if np.array([gb_prep_proba[idx][0], lr_prep_proba[idx][0]]).mean() > 0.5:\n",
    "            writer.writerow([doc[0], 0])\n",
    "        else:\n",
    "            writer.writerow([doc[0], 1])\n",
    "        idx += 1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
